{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21112d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.10\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a217c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Collecting datasets\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/86/27/9c91ddee87b06d2de12f134c5171a49890427e398389f07f6463485723c3/datasets-1.9.0-py3-none-any.whl (262 kB)\n",
      "\u001b[K     |████████████████████████████████| 262 kB 53.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/08/a2/d4e1024c891506e1cee8f9d719d20831bac31cb5b7416983c4d2f65a6287/datasets-1.8.0-py3-none-any.whl (237 kB)\n",
      "\u001b[K     |████████████████████████████████| 237 kB 19.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from datasets) (20.9)\n",
      "Requirement already satisfied: importlib-metadata in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from datasets) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: pandas in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: fsspec in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from datasets) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from datasets) (1.20.2)\n",
      "Collecting huggingface-hub<0.1.0\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/35/03/071adc023c0a7e540cf4652fa9cad13ab32e6ae469bf0cc0262045244812/huggingface_hub-0.0.13-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: filelock in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from packaging->datasets) (2.4.7)\n",
      "Collecting pyarrow<4.0.0,>=1.0.0\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/fc/61/814f4c8d2cd4d51dfd80a9c4ea14b8fd09e37cb0f6962c1f04d504a02e03/pyarrow-3.0.0-cp37-cp37m-manylinux2014_x86_64.whl (20.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.7 MB 714 kB/s eta 0:00:01�███████▎             | 11.8 MB 6.7 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Collecting tqdm<4.50.0,>=4.27\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/73/d5/f220e0c69b2f346b5649b66abebb391df1a00a59997a7ccf823325bd7a3e/tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 19.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jieba\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.2 MB 23.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rouge-score\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from rouge-score) (1.15.0)\n",
      "Collecting transformers\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 77.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from transformers) (5.1)\n",
      "Collecting huggingface-hub<0.1.0\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/1f/f4/2f5fd1d2b7d383d08128d2f79667ab1568f200e21b701eafca7c0075b8cf/regex-2021.7.6-cp37-cp37m-manylinux2014_x86_64.whl (721 kB)\n",
      "\u001b[K     |████████████████████████████████| 721 kB 15.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/92/c9/ef0fae29182d7a867d203f0eff8296b60da92098cc41db33a434f4be84bf/absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 61.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/52/d6/79f40d230895fa1ce3b6af0d22e0ac79c65175dc069c194b79cc8e05a033/dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 20.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.1)\n",
      "Collecting multiprocess\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/db/20/458ac043a57322365ac2ed86a911bf7598fc2e49bccb3f94ea810fbb6b9b/multiprocess-0.70.11.1-py37-none-any.whl (108 kB)\n",
      "\u001b[K     |████████████████████████████████| 108 kB 20.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 85.3 MB/s eta 0:00:01    | 1.1 MB 85.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from nltk->rouge-score) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from nltk->rouge-score) (7.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\n",
      "Collecting sacremoses\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 78.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 15.0 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314477 sha256=f0457a625efad965cc17c3d84bb3255a03ede41a175f1d1f3677982a68b8cc19\n",
      "  Stored in directory: /home/ma-user/.cache/pip/wheels/86/bc/97/67c05f24b07573fd425039f944f8bc57c8dbc6f2c0bcc046fa\n",
      "Successfully built jieba\n",
      "Installing collected packages: tqdm, regex, dill, xxhash, tokenizers, sacremoses, pyarrow, nltk, multiprocess, huggingface-hub, absl-py, transformers, rouge-score, jieba, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.60.0\n",
      "    Uninstalling tqdm-4.60.0:\n",
      "      Successfully uninstalled tqdm-4.60.0\n",
      "Successfully installed absl-py-0.12.0 datasets-1.8.0 dill-0.3.3 huggingface-hub-0.0.12 jieba-0.42.1 multiprocess-0.70.11.1 nltk-3.6.2 pyarrow-3.0.0 regex-2021.7.6 rouge-score-0.0.4 sacremoses-0.0.45 tokenizers-0.10.2 tqdm-4.49.0 transformers-4.8.2 xxhash-2.0.2\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-1.4/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets transformers rouge-score jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce748804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/jupyter-nbextension\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/jupyter_core/application.py\", line 254, in launch_instance\n",
      "    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py\", line 845, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/notebook/nbextensions.py\", line 983, in start\n",
      "    super(NBExtensionApp, self).start()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/jupyter_core/application.py\", line 243, in start\n",
      "    self.subapp.start()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/notebook/nbextensions.py\", line 891, in start\n",
      "    self.toggle_nbextension_python(self.extra_args[0])\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/notebook/nbextensions.py\", line 867, in toggle_nbextension_python\n",
      "    logger=self.log)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/notebook/nbextensions.py\", line 478, in enable_nbextension_python\n",
      "    logger=logger)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/notebook/nbextensions.py\", line 375, in _set_nbextension_state_python\n",
      "    m, nbexts = _get_nbextension_metadata(module)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/notebook/nbextensions.py\", line 1117, in _get_nbextension_metadata\n",
      "    m = import_item(module)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/traitlets/utils/importstring.py\", line 38, in import_item\n",
      "    return __import__(parts[0])\n",
      "ModuleNotFoundError: No module named 'widgetsnbextension'\n",
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: tqdm in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (4.49.0)\n",
      "Collecting ipywidgets\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/11/53/084940a83a8158364e630a664a30b03068c25ab75243224d6b488800d43a/ipywidgets-7.6.3-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipython>=4.0.0 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from ipywidgets) (7.22.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from ipywidgets) (5.3.4)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from ipywidgets) (5.0.5)\n",
      "Requirement already satisfied: jupyter-client in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
      "Requirement already satisfied: tornado>=4.2 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (52.0.0.post20210125)\n",
      "Requirement already satisfied: backcall in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.18)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: pygments in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (2.8.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.2)\n",
      "Collecting jupyterlab-widgets>=1.0.0\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/18/b5/3473d275e3b2359efdf5768e9df95537308b93a31ad94fa92814ac565826/jupyterlab_widgets-1.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 19.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nbformat>=4.2.0\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/e7/c7/dd50978c637a7af8234909277c4e7ec1b71310c13fb3135f3c8f5b6e045f/nbformat-5.1.3-py3-none-any.whl (178 kB)\n",
      "\u001b[K     |████████████████████████████████| 178 kB 18.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipython-genutils in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets) (4.7.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (20.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.17.3)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.10.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Collecting widgetsnbextension~=3.5.0\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/6c/7b/7ac231c20d2d33c445eaacf8a433f4e22c60677eb9776c7c5262d7ddee2d/widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 83.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting notebook>=4.4.1\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/39/b6/8135d31209691cea4f9e8f4d72e495e0184f45f01946539af9facab3110f/notebook-6.4.0-py3-none-any.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 53.0 MB/s eta 0:00:01      | 1.4 MB 53.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyzmq>=17 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (22.0.3)\n",
      "Requirement already satisfied: prometheus-client in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.10.0)\n",
      "Requirement already satisfied: jinja2 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.11.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.1)\n",
      "Collecting Send2Trash>=1.5.0\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/49/46/c3dc27481d1cc57b9385aff41c474ceb7714f7935b1247194adae45db714/Send2Trash-1.5.0-py3-none-any.whl (12 kB)\n",
      "Collecting terminado>=0.8.3\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/7c/07/fa3d6f4c30838acd46a642283a69a5d0498b692c6f42f20d79f64ec63a80/terminado-0.10.1-py3-none-any.whl (14 kB)\n",
      "Collecting tqdm\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/7a/ec/f8ff3ccfc4e59ce619a66a0bf29dc3b49c2e8c07de29d572e191c006eaa2/tqdm-4.61.2-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 94.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting argon2-cffi\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/e0/d7/5da06217807106ed6d7b4f5ccb8ec5e3f9ec969217faad4b5d1af0b55101/argon2_cffi-20.1.0-cp35-abi3-manylinux1_x86_64.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 19.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cffi>=1.0.0\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/46/5e/2f5f83be1586e04f4f14b32c48aafb29197355cca8f62d430f915706fafa/cffi-1.14.6-cp37-cp37m-manylinux1_x86_64.whl (402 kB)\n",
      "\u001b[K     |████████████████████████████████| 402 kB 13.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.1.1)\n",
      "Collecting nbconvert\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/13/2f/acbe7006548f3914456ee47f97a2033b1b2f3daf921b12ac94105d87c163/nbconvert-6.0.7-py3-none-any.whl (552 kB)\n",
      "\u001b[K     |████████████████████████████████| 552 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting entrypoints>=0.2.2\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/ac/c6/44694103f8c221443ee6b0041f69e2740d89a25641e62fb4f2ee568f2f9c/entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
      "Collecting mistune<2,>=0.8.1\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/09/ec/4b43dae793655b7d8a25f76119624350b4d65eb663459eb9603d7f1f0345/mistune-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting nbclient<0.6.0,>=0.5.0\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/22/a6/f3a01a5c1a0e72d1d064f33d4cd9c3a782010f48f48f47f256d0b438994a/nbclient-0.5.3-py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 59.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pandocfilters>=1.4.1\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/28/78/bd59a9adb72fa139b1c9c186e6f65aebee52375a747e4b6a6dcf0880956f/pandocfilters-1.4.3.tar.gz (16 kB)\n",
      "Collecting async-generator\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/71/52/39d20e03abd0ac9159c162ec24b93fbcaa111e8400308f2465432495ca2b/async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting bleach\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/f0/46/2bbd92086a4c6f051214cb48df6d9132b5f32c5e881d3f4991b16ec7e499/bleach-3.3.0-py2.py3-none-any.whl (283 kB)\n",
      "\u001b[K     |████████████████████████████████| 283 kB 15.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (20.9)\n",
      "Collecting defusedxml\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/07/6c/aa3f2f849e01cb6a001cd8554a88d4c77c5c1a31c95bdf1cf9301e6d9ef4/defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/a8/6f/c34288766797193b512c6508f5994b830fb06134fdc4ca8214daba0aa443/jupyterlab_pygments-0.1.2-py2.py3-none-any.whl (4.6 kB)\n",
      "Collecting nest-asyncio\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/52/e2/9b37da54e6e9094d2f558ae643d1954a0fa8215dfee4fa261f31c5439796/nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\n",
      "Collecting pycparser\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/ae/e7/d9c3a176ca4b02024debf82342dab36efadfc5776f9c8db077e8f6e71821/pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 17.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting testpath\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/1b/9e/1a170feaa54f22aeb5a5d16c9015e82234275a3c8ab630b552493f9cb8a9/testpath-0.4.4-py2.py3-none-any.whl (163 kB)\n",
      "\u001b[K     |████████████████████████████████| 163 kB 19.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting webencodings\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: pandocfilters\n",
      "  Building wheel for pandocfilters (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pandocfilters: filename=pandocfilters-1.4.3-py3-none-any.whl size=7992 sha256=5bd89ebf1778aa52a798512f5f832d2493c9f4b6e13e6b99220e850f931d4efe\n",
      "  Stored in directory: /home/ma-user/.cache/pip/wheels/df/3d/89/2398c5b63cfc93a54fc63bb278dd83e4d4892d03279eb616b1\n",
      "Successfully built pandocfilters\n",
      "Installing collected packages: webencodings, pycparser, nest-asyncio, nbformat, async-generator, testpath, pandocfilters, nbclient, mistune, jupyterlab-pygments, entrypoints, defusedxml, cffi, bleach, terminado, Send2Trash, nbconvert, argon2-cffi, notebook, widgetsnbextension, jupyterlab-widgets, tqdm, ipywidgets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.49.0\n",
      "    Uninstalling tqdm-4.49.0:\n",
      "      Successfully uninstalled tqdm-4.49.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 1.8.0 requires tqdm<4.50.0,>=4.27, but you have tqdm 4.61.2 which is incompatible.\u001b[0m\n",
      "Successfully installed Send2Trash-1.5.0 argon2-cffi-20.1.0 async-generator-1.10 bleach-3.3.0 cffi-1.14.6 defusedxml-0.7.1 entrypoints-0.3 ipywidgets-7.6.3 jupyterlab-pygments-0.1.2 jupyterlab-widgets-1.0.0 mistune-0.8.4 nbclient-0.5.3 nbconvert-6.0.7 nbformat-5.1.3 nest-asyncio-1.5.1 notebook-6.4.0 pandocfilters-1.4.3 pycparser-2.20 terminado-0.10.1 testpath-0.4.4 tqdm-4.61.2 webencodings-0.5.1 widgetsnbextension-3.5.1\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-1.4/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade ipywidgets tqdm\n",
    "! jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee9adfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-bbf92655fa2ed704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/ma-user/.cache/huggingface/datasets/json/default-bbf92655fa2ed704/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf4a0ac6fe24ed286c6c1f415cc695b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/ma-user/.cache/huggingface/datasets/json/default-bbf92655fa2ed704/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from ipywidgets import IntProgress\n",
    "import tqdm \n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files='nlpcc_data.json', field='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "214e4f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'content'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b16fd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de072cbb83547b180a61b5576ffff35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def flatten(example):\n",
    "    return {\n",
    "        \"document\": example[\"content\"],\n",
    "        \"summary\": example[\"title\"],\n",
    "        \"id\":\"0\"\n",
    "    }\n",
    "dataset = dataset[\"train\"].map(flatten, remove_columns=[\"title\", \"content\"]) # , remove_columns=[\"title\", \"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89636f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fc7fc6491d416da15025c8e95f7d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8a7675734149ee8881232a38313b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd351263152f4ddcbcffe10765d5bbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455aa144eab1483b8622edae7b4ccb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TokenModel = \"bert-base-chinese\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(TokenModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8c468f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/bart-large-cnn\"\n",
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\" # BART-12-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a6a9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024 # input, source text\n",
    "max_target_length = 256 # summary, target text\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43405ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'id', 'summary'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f2b6ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5402c19a51114974a071064220c3db61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = dataset\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ed62e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59282346a794ecdb63f9244442abdb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7734ac264386415f85b0986c4faeb78a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b4d1129e4d41e19d9f2cdfbfb7ada7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import dataset_dict\n",
    "import datasets\n",
    "\n",
    "train_data_txt, validation_data_txt = dataset.train_test_split(test_size=0.1).values()\n",
    "train_data_txt, test_data_tex = train_data_txt.train_test_split(test_size=0.1).values()\n",
    "# 装载数据\n",
    "dd = datasets.DatasetDict({\"train\":train_data_txt,\"validation\": validation_data_txt,\"test\":test_data_tex }) \n",
    "\n",
    "raw_datasets = dd\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0870df27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'id', 'summary'],\n",
       "        num_rows: 40500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'id', 'summary'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'id', 'summary'],\n",
       "        num_rows: 4500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa338e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f8746a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>id</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>中国天气网讯中央气象台02月01日18时发布寒潮蓝色预警:受强冷空气影响,预计2日至4日,长江中下游及以北地区将先后出现大风降温天气,降温幅度一般有6~10℃,其中内蒙古中部、东北地区中南部、华北北部、黄淮东部等地部分地区降温幅度可达10~14℃,局地可超过14℃,上述地区并伴有4~6级偏北风,江河湖面风力可达5~7级。东部海区将有7~8级大风,阵风可达9级。未来两天具体预报如下:1日20时至2日20时,内蒙古、华北北、东北地区中西部等地气温将下降6~10℃,局地降温10~14℃,局地可超过14℃。2日20时至3日20时,东北、华北中南部、黄淮和江淮等地气温将下降6~8℃,其中,东北地区东部和黄淮东部等地的部分地区降温幅度可达10~12℃,局地可超过12℃。防御指南:1、人员要注意添衣保暖;在生产上做好对大风降温天气的防御准备;2、门窗、围板、棚架、临时搭建物等易被大风吹动的搭建物固紧,妥善安置易受大风影响的室外物品;3、应到避风场所避风,通知户外作业人员注意安全;4、留意有关媒体报道大风降温的最新信息,以便采取进一步措施。</td>\n",
       "      <td>0</td>\n",
       "      <td>寒潮蓝色预警:内蒙、东北、华北等地降温10~14℃。查看降温地图</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>据中央组织部有关负责人证实,国家旅游局副局长、党组成员霍克涉嫌严重违纪违法,中央已决定免去其领导职务。早前报道&gt;&gt;国家旅游局副局长霍克“落马”上任仅仅一个月(来源:新华网)16日晚,中纪委对外通报,国家旅游局副局长、党组成员霍克涉嫌严重违纪违法,目前正接受组织调查。霍克是继国安部副部长马建落马之后,16日通报的第二名落马的中管干部。值得注意的是,霍克履新国家旅游局刚满一个月,此前他长期任职于中央办公厅秘书局。据国家旅游局网站2014年12月16日消息,霍克调任国家旅游局副局长。截至昨天,霍克整整履新一月。记者16日从国家旅游局网站查询获悉,霍克负责综合协调、财务、红色旅游、新闻宣传网络信息管理方面工作。与多数接受调查官员的通告相似,中纪委通报的霍克被调查的消息,尚未具体透露其“涉嫌严重违纪违法”的缘由。但是根据公开资料,今年53岁的霍克曾任北京军区政治部战友报社记者,后来进入中央办公厅秘书局,一步步升任副处长、处长、副局长直至晋升为局长。其最主要的任职经历在中央办公厅。中办是中共中央的综合办事机构。2014年12月22日“落马”的令计划其主要的任职经历即在中共中央办公厅。根据公开简历,霍克的任职履历与令计划存在交集。(记者孙乾)旅游局副局长霍克被查在中办履历与令计划相似(来源:新京报)新京报讯(首席记者王姝记者贾世煜)去年12月16日,霍克从中央办公厅秘书局局长岗位,调任国家旅游局副局长,至昨日其履新恰好满月。昨晚9时55分,中纪委官网发消息称,“国家旅游局副局长、党组成员霍克涉嫌严重违纪违法,目前正接受组织调查”。昨晚,国家旅游局官网“局领导介绍”板块显示,在该局“一正五副”的局领导班子架构中,霍克排在最后一位,其工作分工为“负责综合协调、财务、红色旅游、新闻宣传网络信息管理方面工作。分管办公室、规划财务司、红色旅游办公室等”。其官网中,关于霍克的“重要讲话”、“重要活动”,均是一片空白。初步统计,调任国家旅游局副局长后,霍克只公开亮相了一次。据《甘肃日报》去年12月23日报道,甘肃全省旅游大会举行时,霍克出席了会议并发表讲话。调任国家旅游局之前,关于霍克的公开信息就不多。其仕途履历可以分为两个阶段:军旅生涯、中办经历。霍克生于1961年,河北人。公开履历显示,1976年,年仅15岁的霍克就参加工作,他曾任北京军区某部技术员、干事、教员,曾在解放军南京政治学院新闻系学习,此后他当过北京军区政治部战友报社记者、编辑。之后,霍克进入中央办公厅,并一直在秘书局工作。他从干部做起,历经副处长、处长、副局长等职务,其后走上中央办公厅秘书局局长岗位。公开履历未公布霍克的任职履历年限,但从中央办公厅秘书局普通干部到中央办公厅秘书局局长的“跨越”,意味着霍克在中央办公厅秘书局已工作多年。霍克的上述经历,跟其“老领导”令计划有相似之处。令计划也是从中央办公厅调研室三组负责人做起,历经中央办公厅调研室副主任、调研室主任等职,1999年起任中央办公厅副主任,2007年升任中央办公厅主任,自此直到2012年担任中央统战部部长。9年中央办公厅副主任、5年中央办公厅主任,令计划在中央办公厅工作的这12年间,霍克一直是其下属。霍克去年12月16日从中办调到国家旅游局一周后,中纪委于去年12月23日通报了令计划被调查的消息。</td>\n",
       "      <td>0</td>\n",
       "      <td>中组部证实国家旅游局副局长霍克被免职;其上任仅1个月,此前任中央办公厅秘书局局长,履历与令计划有交集</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>发布日期:2015-07-2311:05:00杭州市气象台2015年7月23日11时05分发布暴雨黄色预警信号:受强对流云团影响,预计今天半夜以前主城区、临安、余杭、萧山短时雨量可达大雨到暴雨,并可能伴有6-8级雷雨大风和强雷电,请注意防范。图例标准防御指南6小时内降雨量将达50毫米以上,或者已达50毫米以上且降雨可能持续。1、政府及相关部门按照职责做好防暴雨工作;2、交通管理部门应当根据路况在强降雨路段采取交通管制措施,在积水路段实行交通引导;3、切断低洼地带有危险的室外电源,暂停在空旷地方的户外作业,转移危险地带人员和危房居民到安全场所避雨;4、检查城市、农田、鱼塘排水系统,采取必要的排涝措施。</td>\n",
       "      <td>0</td>\n",
       "      <td>杭州市发布暴雨黄色预警:受强对流云团影响,预计今天半夜以前主城区、临安、余杭、萧山短时雨量可达大雨到暴雨,并可能...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>过期洗化用品外包装上贴上了商家自制的生产日期标签实习生李帅通讯员韩学军郭中岳文图本报讯过期变质商品贴个自制生产日期标签,重登货架。近日,在汝州市杨楼镇一商场内就发生这样的事情,其商场经营方为让过期变质洗化用品得以出售,便自制新的生产日期标签贴在商品外包装上。目前,工商部门已将这批共113盒过期变质洗化用品查扣,并对商场经营方处以罚款行政处罚。记者从工商局获悉,7月23日上午,一位市民在该商场购物时,发现一批洗化用品的外包装上全贴着纸质打印的生产日期标签,且一揭就掉。随后,这位市民便立即向工商部门举报。接到群众举报后,汝州市工商局执法人员第一时间赶到现场,经检验后,查扣过期洗化用品113盒。汝州市工商局已依据《中华人民共和国产品质量法》等相关规定,对商场经营方处以罚款行政处罚。</td>\n",
       "      <td>0</td>\n",
       "      <td>河南一商家为过期变质商品贴自制标签,重登货架。工商部门已查扣这批洗化用品,并对经营方处以罚款</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>央广网北京11月9日消息(新疆台记者石松)据中国之声《央广新闻》报道,新疆维吾尔自治区首府乌鲁木齐市,现在拥有人口330多万,但是日常实际运行的急救车只有26辆,将近12万人才配有一辆急救车,急救车救人经常迟到。目前,乌鲁木齐急救车和日常需求存在较大差距,急救车“要车难”甚至“要不到车”的问题日趋严重,仅去年,需要急救地点附近无车可调的次数达到了1787次。尽管还没有发生无急救车导致的死亡事件,但按照这种趋势发展下去,情况很不乐观。同时,尽管乌鲁木齐从2010年起在城郊新建了7个120急救站,但由于缺少人员和经费支持,新建站点一直未投入使用。更雪上加霜的是:现有的第二济困医院、新医大五附院、米东区人民医院等医院的急救站也因车辆及车载设备老化处于停运的边缘;西山医院、矿物局医院由于急救人员缺乏,无人员保障已退出急救网络;解放军474医院也暂时申请停运。所以乌鲁木齐急救车经常迟到的现象越来越多。主持人。今年乌鲁木齐正抓紧规划编制12个新建急救站。同时,升级改造“120”调度指挥平台,初步形成以“120”调度指挥中心,29个急救站为平台的网络构架。同时,当地政协委员建议将120急救车辆及装备、一线应急检验检测、储备、急救处置等专项经费开支保障纳入全市民生工程。还建议采取聘用和招募志愿者的方式,逐步解决全市120急救工作中担架员缺乏的问题。针对一线疫病、急救医师等专业人员缺乏现象,要给予优惠、扩大外聘数量,并采取实现医护人员到急救一线轮岗方式,克服人力不足,最大限度动员社会积极力量参与公共卫生应急服务。</td>\n",
       "      <td>0</td>\n",
       "      <td>乌鲁木齐市近12万人才有一辆急救车 救人常迟到</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daf4eb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Requirement already satisfied: transformers in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (4.8.2)\n",
      "Requirement already satisfied: pyyaml in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from transformers) (5.1)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from transformers) (1.20.2)\n",
      "Requirement already satisfied: packaging in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: filelock in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from transformers) (3.10.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from transformers) (2021.7.6)\n",
      "Requirement already satisfied: sacremoses in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from transformers) (4.61.2)\n",
      "Requirement already satisfied: requests in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: click in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-1.4/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "772afe2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4898120860134d6db968dd9a8adde303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22f772bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Collecting fire\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/11/07/a119a1aa04d37bc819940d95ed7e135a7dcca1c098123a3764a6dcace9e7/fire-0.4.0.tar.gz (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 24.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from fire) (1.15.0)\n",
      "Collecting termcolor\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Building wheels for collected packages: fire, termcolor\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=2cfba97335e5735cdd451800454337a8fcf3f9d4f94fed953f18d2e34b2b07e1\n",
      "  Stored in directory: /home/ma-user/.cache/pip/wheels/e8/76/bf/756dec583377017418fc678cb07273c9515a265998532caf1f\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=e02453af358b6400c027530b7325b6e37e5b3df8d32efe1e92eb2ef437b578db\n",
      "  Stored in directory: /home/ma-user/.cache/pip/wheels/5b/cf/22/70db9d77b64938698ed47bd699ec1723d266511829119b7dc2\n",
      "Successfully built fire termcolor\n",
      "Installing collected packages: termcolor, fire\n",
      "Successfully installed fire-0.4.0 termcolor-1.1.0\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-1.4/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12c3849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import fire\n",
    "from torch import nn\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, PreTrainedModel\n",
    "from transformers.utils import logging\n",
    "\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8c74bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_layers(src_layers: nn.ModuleList, dest_layers: nn.ModuleList, layers_to_copy: List[int]) -> None:\n",
    "    layers_to_copy = nn.ModuleList([src_layers[i] for i in layers_to_copy])\n",
    "    assert len(dest_layers) == len(layers_to_copy), f\"{len(dest_layers)} != {len(layers_to_copy)}\"\n",
    "    dest_layers.load_state_dict(layers_to_copy.state_dict())\n",
    "\n",
    "\n",
    "LAYERS_TO_COPY = {\n",
    "    # maps  num layers in teacher -> num_layers in student -> which teacher layers to copy.\n",
    "    # 12: bart, 16: pegasus, 6: marian/Helsinki-NLP\n",
    "    12: {\n",
    "        1: [0],  # This says that if the teacher has 12 layers and the student has 1, copy layer 0 of the teacher\n",
    "        2: [0, 6],\n",
    "        3: [0, 6, 11],      # the first, 7th and 12th decode layers\n",
    "        4: [0, 4, 8, 11],\n",
    "        6: [0, 2, 4, 7, 9, 11],\n",
    "        9: [0, 1, 2, 4, 5, 7, 9, 10, 11],\n",
    "        12: list(range(12)),\n",
    "    },\n",
    "    16: {  # maps  num layers in student -> which teacher layers to copy\n",
    "        1: [0],\n",
    "        2: [0, 15],\n",
    "        3: [0, 8, 15], \n",
    "        4: [0, 5, 10, 15],\n",
    "        6: [0, 3, 6, 9, 12, 15],\n",
    "        8: [0, 2, 4, 6, 8, 10, 12, 15],\n",
    "        9: [0, 1, 3, 5, 7, 9, 11, 13, 15],\n",
    "        12: [0, 1, 2, 3, 4, 5, 6, 7, 9, 11, 13, 15],\n",
    "        16: list(range(16)),\n",
    "    },\n",
    "    6: {1: [0], 2: [0, 5], 3: [0, 2, 5], 4: [0, 1, 3, 5], 6: list(range(6))},\n",
    "}\n",
    "LAYERS_TO_SUPERVISE = {\n",
    "    # maps  num layers in student -> which teacher layers to copy.\n",
    "    6: {1: [5], 2: [3, 5], 3: [1, 4, 5], 4: [1, 2, 4, 5]},\n",
    "    12: {1: [11], 2: [5, 11], 3: [3, 7, 11], 6: [1, 3, 5, 8, 10, 11]},\n",
    "    16: {1: [15], 4: [4, 9, 12, 15], 8: [1, 3, 5, 7, 9, 11, 13, 15]},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8ac00d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_student_by_copying_alternating_layers(\n",
    "    teacher: Union[str, PreTrainedModel],\n",
    "    save_path: Union[str, Path] = \"student\",\n",
    "    e: Union[int, None] = None,\n",
    "    d: Union[int, None] = None,\n",
    "    copy_first_teacher_layers=False,\n",
    "    e_layers_to_copy=None,\n",
    "    d_layers_to_copy=None,\n",
    "    **extra_config_kwargs\n",
    ") -> Tuple[PreTrainedModel, List[int], List[int]]:\n",
    "    \n",
    "    _msg = \"encoder_layers and decoder_layers cannot be both None-- you would just have an identical teacher.\"\n",
    "    assert (e is not None) or (d is not None), _msg\n",
    "    if isinstance(teacher, str):\n",
    "        AutoTokenizer.from_pretrained(teacher).save_pretrained(save_path)  # purely for convenience\n",
    "        teacher = AutoModelForSeq2SeqLM.from_pretrained(teacher).eval()\n",
    "    else:\n",
    "\n",
    "        assert isinstance(teacher, PreTrainedModel), f\"teacher must be a model or string got type {type(teacher)}\"\n",
    "    init_kwargs = teacher.config.to_diff_dict()\n",
    "\n",
    "    try:\n",
    "        teacher_e, teacher_d = teacher.config.encoder_layers, teacher.config.decoder_layers\n",
    "        if e is None:\n",
    "            e = teacher_e\n",
    "        if d is None:\n",
    "            d = teacher_d\n",
    "        init_kwargs.update({\"encoder_layers\": e, \"decoder_layers\": d})\n",
    "    except AttributeError:  # T5\n",
    "        teacher_e, teacher_d = teacher.config.num_layers, teacher.config.num_decoder_layers\n",
    "        if e is None:\n",
    "            e = teacher_e\n",
    "        if d is None:\n",
    "            d = teacher_d\n",
    "        init_kwargs.update({\"num_layers\": e, \"num_decoder_layers\": d})\n",
    "\n",
    "    # Kwargs to instantiate student: teacher kwargs with updated layer numbers + **extra_config_kwargs\n",
    "    init_kwargs.update(extra_config_kwargs)\n",
    "\n",
    "    # Copy weights\n",
    "    student_cfg = teacher.config_class(**init_kwargs)\n",
    "    student = AutoModelForSeq2SeqLM.from_config(student_cfg)\n",
    "    # Start by copying the full teacher state dict this will copy the first N teacher layers to the student.\n",
    "    info = student.load_state_dict(teacher.state_dict(), strict=False)\n",
    "    assert info.missing_keys == [], info.missing_keys  # every student key should have a teacher keys.\n",
    "\n",
    "    if copy_first_teacher_layers:  # Our copying is done. We just log and save\n",
    "        e_layers_to_copy, d_layers_to_copy = list(range(e)), list(range(d))\n",
    "        logger.info(\n",
    "            f\"Copied encoder layers {e_layers_to_copy} and decoder layers {d_layers_to_copy}. Saving them to {save_path}\"\n",
    "        )\n",
    "        student.save_pretrained(save_path)\n",
    "        return student, e_layers_to_copy, d_layers_to_copy\n",
    "\n",
    "    # Decide which layers of the teacher to copy. Not exactly alternating -- we try to keep first and last layer.\n",
    "    if e_layers_to_copy is None:\n",
    "        e_layers_to_copy: List[int] = pick_layers_to_copy(e, teacher_e)\n",
    "    if d_layers_to_copy is None:\n",
    "        d_layers_to_copy: List[int] = pick_layers_to_copy(d, teacher_d)\n",
    "\n",
    "    try:\n",
    "        copy_layers(teacher.model.encoder.layers, student.model.encoder.layers, e_layers_to_copy)\n",
    "        copy_layers(teacher.model.decoder.layers, student.model.decoder.layers, d_layers_to_copy)\n",
    "    except AttributeError:  # For t5, student.model.encoder.layers is called student.encoder.block\n",
    "        copy_layers(teacher.encoder.block, student.encoder.block, e_layers_to_copy)\n",
    "        copy_layers(teacher.decoder.block, student.decoder.block, d_layers_to_copy)\n",
    "    logger.info(\n",
    "        f\"Copied encoder layers {e_layers_to_copy} and decoder layers {d_layers_to_copy}. Saving them to {save_path}\"\n",
    "    )\n",
    "    student.config.init_metadata = dict(\n",
    "        teacher_type=teacher.config.model_type,\n",
    "        copied_encoder_layers=e_layers_to_copy,\n",
    "        copied_decoder_layers=d_layers_to_copy,\n",
    "    )\n",
    "    student.save_pretrained(save_path)\n",
    "    # Save information about copying for easier reproducibility\n",
    "\n",
    "    return student, e_layers_to_copy, d_layers_to_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3add8c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_layers_to_copy(n_student, n_teacher):\n",
    "    try:\n",
    "        val = LAYERS_TO_COPY[n_teacher][n_student]\n",
    "        return val\n",
    "    except KeyError:\n",
    "        if n_student != n_teacher:\n",
    "            warnings.warn(\n",
    "                f\"no hardcoded layers to copy for teacher {n_teacher} -> student {n_student}, defaulting to first {n_student}\"\n",
    "            )\n",
    "        return list(range(n_student))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb003d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, list_en, list_de = create_student_by_copying_alternating_layers(model, 'trian.pth', 12, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9cf07ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    num_train_epochs=1,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=batch_size,  # demo\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    # learning_rate=3e-05,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b58a54f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa3a3f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(jieba.cut(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(jieba.cut(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25d5b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b93fe4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Collecting torch==1.5.0\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/76/58/668ffb25215b3f8231a550a227be7f905f514859c70a65ca59d28f9b7f60/torch-1.5.0-cp37-cp37m-manylinux1_x86_64.whl (752.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 752.0 MB 14.5 MB/s eta 0:00:01MB 68.6 MB/s eta 0:00:11 MB 68.6 MB/s eta 0:00:11\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from torch==1.5.0) (1.20.2)\n",
      "Requirement already satisfied: future in /home/ma-user/anaconda3/envs/PyTorch-1.4/lib/python3.7/site-packages (from torch==1.5.0) (0.18.2)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.4.0\n",
      "    Uninstalling torch-1.4.0:\n",
      "      Successfully uninstalled torch-1.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.5.0 requires torch==1.4.0, but you have torch 1.5.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.5.0\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-1.4/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install torch==1.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cd0e92de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id, token_type_ids, document, summary.\n",
      "***** Running training *****\n",
      "  Num examples = 40500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 20250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20250' max='20250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20250/20250 1:33:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>12.439500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>6.301900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.089800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>6.076500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.507800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.921700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.299200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>5.257500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.245900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>4.773800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.916500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.795400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.774400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.688500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>4.498900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.658300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>4.586300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.511600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>4.631400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>4.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>4.306700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.325700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>4.495000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>4.234200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>4.353300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>4.243100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>4.726900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.435300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>4.248200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>4.455300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>4.306300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>4.336500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>4.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>4.242400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>4.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>4.441200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>4.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.171600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>4.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>4.274400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>4.135400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>4.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>4.317500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>4.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>4.199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>4.079000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>3.998700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>4.305700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>4.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>4.085600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>4.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>4.195500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>4.276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>4.075300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>4.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>4.063900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>4.107900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>3.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>4.317000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>4.190500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>4.193700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>4.108100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>3.737000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>4.033400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>3.909800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>3.926300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>4.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>3.979200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>4.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>3.789500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>3.895100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>3.852700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>3.939300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>3.859500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.960900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>3.788400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>3.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>3.878300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>4.221100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>3.866000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>3.906000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>4.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>3.968100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>3.951500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>3.989200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>4.136500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>3.972900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>3.897500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>3.995700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>3.783800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>3.964500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>3.985100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>3.918800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.897200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>3.816700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>4.008700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>3.855600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>3.827900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>3.707800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>4.075300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>3.805700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>3.662900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>3.956800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.990100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>3.711200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>3.937400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>3.839900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>3.829800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>3.838500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>3.737900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>3.842400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>3.610200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>3.783300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.857600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>3.738200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>3.886100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>3.871300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>3.669100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>3.747100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>3.821200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>3.816800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>3.809500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>3.736600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.755500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>3.722500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>3.732900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>3.544100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>3.879800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>3.719000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>3.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>3.809300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>3.762200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>3.592600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.691100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>3.919400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>3.669400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>3.733000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>3.703400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>4.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>3.708300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>3.538400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>3.721800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>3.849100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.591500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>3.655400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>3.693300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>3.879600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>3.559600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>3.818800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>4.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>3.737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>3.793300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>3.726400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.986700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>3.767900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>3.791100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>3.829200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>3.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>3.826500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>3.841100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>3.722600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>3.650300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>3.618800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.894300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>3.537300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>3.585400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>3.532500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>3.734600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>3.672700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>3.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>3.545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>3.682200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>3.836300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.715200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>3.711400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>3.779800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>3.558200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>3.825400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>3.776900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>3.783100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>3.705900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>3.836300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>3.506400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.677400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>3.496500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>3.545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>3.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>3.536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>3.721800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>3.633600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>3.889500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>3.593800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>3.658900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.574400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10050</td>\n",
       "      <td>3.661400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>3.748400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10150</td>\n",
       "      <td>3.596400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>3.610300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>3.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>3.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10350</td>\n",
       "      <td>3.541700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>3.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10450</td>\n",
       "      <td>3.518600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.488400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10550</td>\n",
       "      <td>3.702600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>3.517900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10650</td>\n",
       "      <td>3.626400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>3.783900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>3.618800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>3.655500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10850</td>\n",
       "      <td>3.638800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>3.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>3.621200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.477600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11050</td>\n",
       "      <td>3.567100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>3.625500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11150</td>\n",
       "      <td>3.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>3.554400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>3.623700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>3.519200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11350</td>\n",
       "      <td>3.365200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>3.522900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11450</td>\n",
       "      <td>3.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>3.537200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11550</td>\n",
       "      <td>3.581300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>3.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11650</td>\n",
       "      <td>3.689000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>3.448700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>3.567000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>3.658400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11850</td>\n",
       "      <td>3.651800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>3.403700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11950</td>\n",
       "      <td>3.716200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12050</td>\n",
       "      <td>3.640400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>3.566400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12150</td>\n",
       "      <td>3.552500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>3.550100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>3.653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>3.580100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12350</td>\n",
       "      <td>3.632200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>3.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12450</td>\n",
       "      <td>3.468400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>3.592900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12550</td>\n",
       "      <td>3.679500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>3.705500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12650</td>\n",
       "      <td>3.608600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>3.394900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12750</td>\n",
       "      <td>3.553700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>3.527200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12850</td>\n",
       "      <td>3.423800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>3.536100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12950</td>\n",
       "      <td>3.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.594600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13050</td>\n",
       "      <td>3.460500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>3.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13150</td>\n",
       "      <td>3.501200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>3.377200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13250</td>\n",
       "      <td>3.691300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>3.444800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13350</td>\n",
       "      <td>3.481300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>3.638700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13450</td>\n",
       "      <td>3.508200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>3.423500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13550</td>\n",
       "      <td>3.428200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>3.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13650</td>\n",
       "      <td>3.411300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>3.477100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13750</td>\n",
       "      <td>3.528900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>3.488700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13850</td>\n",
       "      <td>3.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>3.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13950</td>\n",
       "      <td>3.682500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.452400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14050</td>\n",
       "      <td>3.441700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>3.509600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14150</td>\n",
       "      <td>3.555100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>3.536900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14250</td>\n",
       "      <td>3.385000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>3.571700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14350</td>\n",
       "      <td>3.726900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>3.606600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14450</td>\n",
       "      <td>3.487900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>3.659400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14550</td>\n",
       "      <td>3.363200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>3.340400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14650</td>\n",
       "      <td>3.475800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>3.479000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14750</td>\n",
       "      <td>3.636200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>3.615700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14850</td>\n",
       "      <td>3.543400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>3.495600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14950</td>\n",
       "      <td>3.445400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15050</td>\n",
       "      <td>3.653200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>3.587000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15150</td>\n",
       "      <td>3.482700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>3.297300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15250</td>\n",
       "      <td>3.365400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>3.501800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15350</td>\n",
       "      <td>3.486000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>3.521200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15450</td>\n",
       "      <td>3.378800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>3.318400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15550</td>\n",
       "      <td>3.645500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>3.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15650</td>\n",
       "      <td>3.485300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>3.378500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15750</td>\n",
       "      <td>3.508800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>3.410600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15850</td>\n",
       "      <td>3.576200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>3.462800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15950</td>\n",
       "      <td>3.431900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.467400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16050</td>\n",
       "      <td>3.457100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>3.583900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16150</td>\n",
       "      <td>3.411400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>3.283600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16250</td>\n",
       "      <td>3.476200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>3.405500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16350</td>\n",
       "      <td>3.378700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>3.708900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16450</td>\n",
       "      <td>3.599600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>3.383900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16550</td>\n",
       "      <td>3.410800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>3.382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16650</td>\n",
       "      <td>3.502900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>3.575800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16750</td>\n",
       "      <td>3.416900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>3.496300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16850</td>\n",
       "      <td>3.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>3.299800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16950</td>\n",
       "      <td>3.461200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.490700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17050</td>\n",
       "      <td>3.326400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>3.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17150</td>\n",
       "      <td>3.556400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>3.526200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17250</td>\n",
       "      <td>3.556700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>3.368100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17350</td>\n",
       "      <td>3.446100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>3.331600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17450</td>\n",
       "      <td>3.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>3.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17550</td>\n",
       "      <td>3.425600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>3.377900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17650</td>\n",
       "      <td>3.488700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>3.264100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17750</td>\n",
       "      <td>3.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>3.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17850</td>\n",
       "      <td>3.460400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>3.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17950</td>\n",
       "      <td>3.480600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.341000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18050</td>\n",
       "      <td>3.377500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>3.466800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18150</td>\n",
       "      <td>3.426300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>3.395700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18250</td>\n",
       "      <td>3.293800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>3.333000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18350</td>\n",
       "      <td>3.432400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>3.422700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18450</td>\n",
       "      <td>3.496300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>3.435900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18550</td>\n",
       "      <td>3.478400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>3.460200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18650</td>\n",
       "      <td>3.373800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>3.540800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18750</td>\n",
       "      <td>3.324300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>3.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18850</td>\n",
       "      <td>3.432100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>3.327300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18950</td>\n",
       "      <td>3.429100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19050</td>\n",
       "      <td>3.365100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>3.567200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19150</td>\n",
       "      <td>3.411500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>3.561200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19250</td>\n",
       "      <td>3.480300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>3.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19350</td>\n",
       "      <td>3.562600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>3.339300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19450</td>\n",
       "      <td>3.354700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>3.392700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19550</td>\n",
       "      <td>3.415200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.366100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19650</td>\n",
       "      <td>3.244200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>3.236500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19750</td>\n",
       "      <td>3.296400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>3.397800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19850</td>\n",
       "      <td>3.353000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>3.270100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19950</td>\n",
       "      <td>3.334700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.513500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20050</td>\n",
       "      <td>3.403700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>3.387700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20150</td>\n",
       "      <td>3.242600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>3.401800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20250</td>\n",
       "      <td>3.291400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-500\n",
      "Configuration saved in results/checkpoint-500/config.json\n",
      "Model weights saved in results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-500/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-1000\n",
      "Configuration saved in results/checkpoint-1000/config.json\n",
      "Model weights saved in results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to results/checkpoint-1500\n",
      "Configuration saved in results/checkpoint-1500/config.json\n",
      "Model weights saved in results/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to results/checkpoint-2000\n",
      "Configuration saved in results/checkpoint-2000/config.json\n",
      "Model weights saved in results/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-2500\n",
      "Configuration saved in results/checkpoint-2500/config.json\n",
      "Model weights saved in results/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-3000\n",
      "Configuration saved in results/checkpoint-3000/config.json\n",
      "Model weights saved in results/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-3500\n",
      "Configuration saved in results/checkpoint-3500/config.json\n",
      "Model weights saved in results/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-4000\n",
      "Configuration saved in results/checkpoint-4000/config.json\n",
      "Model weights saved in results/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-4500\n",
      "Configuration saved in results/checkpoint-4500/config.json\n",
      "Model weights saved in results/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-5000\n",
      "Configuration saved in results/checkpoint-5000/config.json\n",
      "Model weights saved in results/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-5500\n",
      "Configuration saved in results/checkpoint-5500/config.json\n",
      "Model weights saved in results/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-6000\n",
      "Configuration saved in results/checkpoint-6000/config.json\n",
      "Model weights saved in results/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-6500\n",
      "Configuration saved in results/checkpoint-6500/config.json\n",
      "Model weights saved in results/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-7000\n",
      "Configuration saved in results/checkpoint-7000/config.json\n",
      "Model weights saved in results/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-7500\n",
      "Configuration saved in results/checkpoint-7500/config.json\n",
      "Model weights saved in results/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-8000\n",
      "Configuration saved in results/checkpoint-8000/config.json\n",
      "Model weights saved in results/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-8500\n",
      "Configuration saved in results/checkpoint-8500/config.json\n",
      "Model weights saved in results/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-9000\n",
      "Configuration saved in results/checkpoint-9000/config.json\n",
      "Model weights saved in results/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-9500\n",
      "Configuration saved in results/checkpoint-9500/config.json\n",
      "Model weights saved in results/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-10000\n",
      "Configuration saved in results/checkpoint-10000/config.json\n",
      "Model weights saved in results/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-10500\n",
      "Configuration saved in results/checkpoint-10500/config.json\n",
      "Model weights saved in results/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-11000\n",
      "Configuration saved in results/checkpoint-11000/config.json\n",
      "Model weights saved in results/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-11500\n",
      "Configuration saved in results/checkpoint-11500/config.json\n",
      "Model weights saved in results/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-12000\n",
      "Configuration saved in results/checkpoint-12000/config.json\n",
      "Model weights saved in results/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-12500\n",
      "Configuration saved in results/checkpoint-12500/config.json\n",
      "Model weights saved in results/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-13000\n",
      "Configuration saved in results/checkpoint-13000/config.json\n",
      "Model weights saved in results/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-13500\n",
      "Configuration saved in results/checkpoint-13500/config.json\n",
      "Model weights saved in results/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-14000\n",
      "Configuration saved in results/checkpoint-14000/config.json\n",
      "Model weights saved in results/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-14500\n",
      "Configuration saved in results/checkpoint-14500/config.json\n",
      "Model weights saved in results/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-15000\n",
      "Configuration saved in results/checkpoint-15000/config.json\n",
      "Model weights saved in results/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-15500\n",
      "Configuration saved in results/checkpoint-15500/config.json\n",
      "Model weights saved in results/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-16000\n",
      "Configuration saved in results/checkpoint-16000/config.json\n",
      "Model weights saved in results/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-16000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-16500\n",
      "Configuration saved in results/checkpoint-16500/config.json\n",
      "Model weights saved in results/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-16500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-17000\n",
      "Configuration saved in results/checkpoint-17000/config.json\n",
      "Model weights saved in results/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-17000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-17500\n",
      "Configuration saved in results/checkpoint-17500/config.json\n",
      "Model weights saved in results/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-17500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-18000\n",
      "Configuration saved in results/checkpoint-18000/config.json\n",
      "Model weights saved in results/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-18000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-18500\n",
      "Configuration saved in results/checkpoint-18500/config.json\n",
      "Model weights saved in results/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-18500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-19000\n",
      "Configuration saved in results/checkpoint-19000/config.json\n",
      "Model weights saved in results/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-19000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-19500\n",
      "Configuration saved in results/checkpoint-19500/config.json\n",
      "Model weights saved in results/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-19500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-20000\n",
      "Configuration saved in results/checkpoint-20000/config.json\n",
      "Model weights saved in results/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in results/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in results/checkpoint-20000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-18500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20250, training_loss=3.7772197069709685, metrics={'train_runtime': 5608.721, 'train_samples_per_second': 7.221, 'train_steps_per_second': 3.61, 'total_flos': 5.519068250185728e+16, 'train_loss': 3.7772197069709685, 'epoch': 1.0})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3a810e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5294a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"BART.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6caab61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model.load_state_dict(torch.load('BART.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f604ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29496448",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = \"2005年6月，习近平同志首次提出“红船精神”，将其概括为“开天辟地、敢为人先的首创精神，坚定理想、百折不挠的奋斗精神，立党为公、忠诚为民的奉献精神”，深刻阐述了“红船精神”的丰富内涵、历史地位、时代价值。2017年10月，党的十九大闭幕仅一周，习近平总书记就带领中共中央政治局常委同志，瞻仰上海中共一大会址和浙江嘉兴南湖红船，回顾建党历史，重温入党誓词。习近平总书记在南湖革命纪念馆参观时指出：“我们要结合时代特点大力弘扬‘红船精神’。”“红船精神”是中国革命精神之源，激励着我们党砥砺前行、发展壮大，是我们党立党兴党、执政兴国的宝贵精神财富。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b83e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = generate_summary(test_samples, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ad6d2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[unused2] 月 睾 习 近 平 同 志 首 次 提 出 红 船 精 神 睾 将 其 概 括 为 开 天 辟 地 、 敢 为 人 先 的 首 创 精 神 睾 立 党 为 公 、 忠 诚 为 民 的 奉 献 精 神 。... 。 现 场 图 ) 组 图 ( 组 图 : ) 图 图 ).. 6 图 : 习 6. 。 组 组 图 ) 组 图 图 ) 。 图 图 组 原 金 金 重 。 现 现 场 上 海 [unused2]'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "36aeee13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['重庆:妈妈半斤白酒下肚后喂奶灌醉宝宝,孩子吃完奶皮肤发烫、手脚乱动,吐奶后清醒过来。',\n",
       " '许昌住院女子与男护士发生关系 结婚后不如意状告院方',\n",
       " '资阳原市委书记李佳涉嫌受贿罪被逮捕,案件侦查工作正在进行中。',\n",
       " '遵义气象台20时55分发布雷电黄色预警:目前遵义县、桐梓等地出现雷雨天气,预计未来将影响中东部地区。',\n",
       " '海南藏族自治州发布暴雨橙色预警:预计共和地区3小时内降雨量将达25毫米以上且降雨量可能持续,引发洪涝、泥石流、城...',\n",
       " '昆明一母亲买菜留孩子一人在家,2岁半娃娃4楼坠下气息奄奄已不能哭闹说话;孩子目前已送医仍在抢救。',\n",
       " '厦门23岁女生去福州看同学,搭乘出租车后失踪,昨晚该女生手机始终占线',\n",
       " '长江翻船浙江籍乘客儿女:爸妈,求你们再更新一次朋友圈;11名乘客家属到达监利,盼看老人最后一眼',\n",
       " '酒泉市发布道路结冰黄色预警:预计未来72小时内,我市肃州区仍有道路结冰,请注意防范。...',\n",
       " '国资委:唐复平不再担任鞍钢总经理,杨华不再担任鞍钢党委书记等,姚林兼任鞍钢党委书记和总经理。',\n",
       " '新疆边境一线反恐战斗现场曝光:一伙暴徒准备逃往国外参加“圣战”,武警获取情报主动出击,经激烈枪战歼灭6名暴徒。',\n",
       " '歌手陈红遭前夫起诉,被指以军人身份经商;前夫曾将名下9家公司股权半数转让给陈红,现要求收回。 ',\n",
       " '菲考虑解雇16名中国专家 ,称为确保“国家电网安全”,应改聘菲律宾人;报道称中国专家须本周离开。',\n",
       " '外交部发言人:中国赴菲律宾第一批救援人员将于20日启程',\n",
       " '南京给试点汽车尾气管\"戴口罩\" PM2.5浓度降九成',\n",
       " '西甲:梅西内马尔传射,巴萨3-0完胜埃瓦尔,4分优势领跑,下轮将客场迎战皇马。',\n",
       " '厦门:男子扬言携炸弹上公交车 惊动排爆特警',\n",
       " '覃塘区平天山阴坑一处山体因暴雨发生坍塌,致山脚下一工棚被冲垮4人被埋,其中2人获救,另2人失踪。',\n",
       " '招远血案被害人家属至今未获赔偿,生活陷困顿;多次咨询政府未获实质性说法,称觉得自己已成“乞丐”,整天去政府要钱',\n",
       " '延安:“被逼卖处”嫌犯家属否认央求官员帮忙调解,称“封口费与我无关”;当地政府承认未向当事人了解情况',\n",
       " '侯树森不再担任解放军副总参谋长职务,空军将领乙晓光接替其工作',\n",
       " '<教养>《魔法亲亲》、《一口袋的吻》……这些绘本光看名字心里就暖,帮娃克服入园恐惧。',\n",
       " '英国情侣借助在世界各地帮人看管房屋和照顾宠物,4年游历8国;曾照看羊驼,免费入住海岛别墅',\n",
       " '宋承宪晒和两合作女演员亲密照,遭批利用刘亦菲增加知名度;网友痛骂“宋渣”“花心”',\n",
       " '中甲综述:延边大胜继续领跑,青岛德比中能双杀海牛,毅腾逆转升班马,卓尔告负掉出冲超集团',\n",
       " '深圳居住证申领门槛将提高,符合条件持证人可申请公租房或租房补贴。',\n",
       " '清华大学与美国华盛顿大学、微软合作创建全球创新学院,系中国高校首次在美国办学;将在全球招生,联合授予两校学位。',\n",
       " '组图:西安西七路一餐厅今晨发生火灾,造成3人死亡,目前现场已被封闭,原因正在调查中。',\n",
       " '宁波一男子因好奇台风“长”啥样,不顾风雨深入山中围观“灿鸿”,被困水流多时险些丧命,幸被一驴友发现并发出求救信号救下。',\n",
       " '甘孜一女科员享副县级待遇被疑官二代 官方:已49岁',\n",
       " '日首相安倍晋三确定8天访美行程,计划在美国国会发表演说,并前往4个城市;日媒称此次访美“时间之长实属罕见”。 ',\n",
       " '石家庄一化工厂两声巨响后腾起浓烟,现场存有特殊化工原料。',\n",
       " '桂林:男子超市内劫持女顾客,与警察对峙2小时后被擒,该男子今年5月刑满释放',\n",
       " '媒体称美团正与投行接触,计划新一轮10亿美元的融资,此轮融资对美团估值将超150亿美元。',\n",
       " '香港:凌晨3时,大屿山欣澳倒扣湾一摄制组疑遇风浪船只翻侧,1人死亡,其余7人上岸后不知所踪。',\n",
       " '东营率先启动城镇医保普通门诊统筹,参保人员可从72家普通门诊统筹签约医疗机构中选一家签约,每年最高报销1000元。',\n",
       " '重庆一劳务公司以高薪海外务工为诱惑,收取近千打工者中介服务费用后人间蒸发,涉及金额上亿元。',\n",
       " '淄博:为会网友入室盗窃 一个烟头揪出嫌疑人',\n",
       " '外媒称中国或填海造岛引印度不安;此前马尔代夫修宪,允许投资10亿美元以上的外国人拥有该国土地',\n",
       " '保定市民谈承接首都职能:得看人家愿不愿意来',\n",
       " '江西布暴雨黄色预警信号:未来6小时内,萍乡、新余、鹰潭、南昌、宜春、上饶等地将出现短时强降水、雷雨大风等强对流天气。',\n",
       " 'NBA快讯:哈登51分火箭力克国王 考神超级3双难阻3连败',\n",
       " '土耳其一对新婚夫妇在婚宴上款待4000名叙利亚难民,所有资金来自宾客随礼;叙利亚内战以来已有400万国民出国避难',\n",
       " '中国将建不动产统一登记制 专家称有助征税和反腐',\n",
       " '中国时速500公里动车组明日试验,将超越目前国内所有动车组速度;西南交大停电通知披露消息',\n",
       " '今日上午9时许,安顺一水泥厂在清理磨水渣仓时发生一起事故,造成2人死亡。',\n",
       " '曝托比亚斯-哈里斯与魔术队达成了4年6400万美元的续约协议,此前尼克斯和湖人等都对他感兴趣',\n",
       " '检方回应“秦皇岛亿元贪官母亲否认儿子贪污”:不可能,已掌握相关证据,待案件侦办完会向社会公布',\n",
       " '加勒比小国靠卖国籍吸金:中国、俄罗斯及中东富豪成推销对象,只要付出25万美元即可成为公民,凭此该国获得庞大收入。 ',\n",
       " '吉林市现“僵尸车”被困“牢笼”全身没漆',\n",
       " '广州14岁女初中生在校中毒身亡,警方调查怀疑系自杀;目击者称女生死前曾搅拌水杯。',\n",
       " '宝山金盛公寓居民逢下雨都犯愁屋顶渗水,天花板被腐蚀开裂,墙面霉迹斑斑;小区物业消失开发商失联。',\n",
       " '陌陌回应网易炮轰:唐岩称指控是恶意行为,将积极展开自我辩护;陌陌表示任何法律行动都会造成负面影响。',\n",
       " '组图:9-11恐怖袭击后,布什政府主要成员反应的照片公布,照片真实反映了美国高层官员的震惊和恐惧。',\n",
       " '哆啦A梦中最受关注的配角竟然是大雄的老师?!头条指数为你揭秘!',\n",
       " '结婚十周年妻子车祸丧生,丈夫空买钻戒无人可送;10岁儿子8岁女儿不知母亲已逝,仍画画作母亲节礼物',\n",
       " '人社部称要建立公务员基本工资标准正常调整机制;6月底前近1400万名公务员工资将落实调整。',\n",
       " '英超:曼联主场0-1南安普敦,塔迪奇打入制胜球;曼联10轮不败终结,南安普敦积分反超曼联重回前3',\n",
       " '苏州一男子因嫌发小太过粘人,冲动之下将其砸死抛尸树林;发小极度依赖超出“哥们”承受范围令其精神崩溃。',\n",
       " '组图:江苏无锡一女司机将奔驰车开入路边河道里,所幸及时爬出天窗逃生,被周围群众和消防官兵救出。',\n",
       " '浙江余姚外地居民与志愿者起冲突 救援物资遭哄抢',\n",
       " '据报道,郑州新郑机场一架飞机降落时坠地,机头下陷与地面摩擦出很大火花,滑行数几公里,120、119人员到达现场。',\n",
       " '岳阳:洞庭湖万亩湿地被高价拍卖,“候鸟天堂”被改造成养蟹池塘;拍卖时有人出540万,成交价却只有370万。',\n",
       " '中国证券业协会官网被黑,跳转到赌场,回应称已注意到该现象,正在处理解决',\n",
       " '解放军炮兵、步战车赴中缅边境集结现场曝光,空军等将实弹演习;专家称演习不是秀肌肉,并非针对缅政府',\n",
       " '河池市发布暴雨橙色预警:金城江区3小时降雨量已达50毫米且降雨持续。...',\n",
       " '扬州一家三口饭后中毒,10岁女儿身亡,警方不排除人为投毒;3人曾喝两亲戚买来饮料,两亲戚没喝无异常。',\n",
       " '琼海一孕妇腹痛去医院就诊,打完点滴后出现抽筋呕吐症状,两小时后胎死腹中;医生称死因需要由专家定论。',\n",
       " '保定蠡县“爆粗口”村支书被免,此前在受访时扬言扇死记者,称自己带的村委会班子“一个人好人没有”。',\n",
       " '四川一厅官落马前多次斥责举报者:你们是在反对共产党;曾多次晒与郭永祥、李崇禧等“大老虎”合照',\n",
       " '烟台:男子驾车撞六旬环卫工后弃车逃逸,环卫工当场身亡;警方3小时破案,抓获肇事司机,事故仍在调查中。',\n",
       " '四川古蔺现密集恐龙脚印化石,疑还原亿万年前猎杀场景;专家称脚印或是翼龙所留,有待进一步考证(组图)',\n",
       " '普京称,今年俄罗斯核力量将获得40多枚新型洲际弹道导弹,这些导弹能突破技术最完善的导弹防御系统。 ',\n",
       " '安庆:宿松县五里乡废弃屋内现六具尸骸追踪 诸多疑问待解',\n",
       " '501吨“神龟”落户曹妃甸,将成环渤海区域旅游文化新地标,本月11日前免费开放',\n",
       " '袁隆平回应安徽万亩“超级稻”减产绝收:推广多年品种有可能退化,出点小问题,不能说都有问题',\n",
       " '快讯:日本著名演员高仓健10日因病去世,享年83岁。',\n",
       " '“表妹”实为景春华情妇,开公司倒卖用地指标,后被带走;景任衡水书记后出门常警车开道。',\n",
       " '培生公司同意以7.3亿美元出售经济学人集团50%股份,买家为意大利阿涅利家族控股公司Exor。',\n",
       " '亚足联裁判委员会主席将由张吉龙担任,中国足协的多位候选人确定出任亚足联核心部门领导',\n",
       " '万达回应总部迁往上海:内部还未正式公开文件;王健林曾提出计划将上海万达广场从7个拓展到20个',\n",
       " '中央巡视组:文化部直属企业经营混乱,评奖过滥且存在暗箱操作;环保部领导开公司揽项目牟利;工商联专项资金使用存在问题。',\n",
       " '新西兰恒天然乳粉检出肉毒杆菌 部分流入中国',\n",
       " '松原直属库主任邓安福、开原直属库原主任尤连春等被行政记过、撤职;涉嫌犯罪移交司法部门',\n",
       " '临沂:女子集市遇抢劫,劫匪冒充其老公喊“还不回家”,围观人群以为夫妻打架未阻止。',\n",
       " '曝工行快捷支付有重大漏洞:犯罪分子强行给储户开通快捷支付,通过截获短信验证码盗取存款;6月以来多位北京储户存款被盗。',\n",
       " '达州:乡村“盗墓笔记”十年频现盗尸 24小时守墓仍被盗',\n",
       " '河北民警牺牲后妻子跳楼,两人系同学;儿子探望遗体哭晕,曾目睹母亲跳楼;媒体追问打死警察的枪哪里来。',\n",
       " '成都机场遇大雾,约9000名旅客行程受阻;预计延误航班将在16时恢复正常',\n",
       " '重庆年轻母亲为救4岁白血病女儿卖“拥抱”:10元抱一次,首日收600多元;回应市民质疑称实属无奈',\n",
       " '乐山:女子在大巴车被邻座男子多次抚摸,辩解不小心碰到;一怒之下扇其耳光:老娘30多岁,是摸还是碰能不知道?',\n",
       " '黔南布依族苗族自治州发布暴雨黄色预警: 根据最新监测资料分析,目前我州惠水、贵定、龙里个别乡镇降水量已达到50m...',\n",
       " '厦门:男子输光积蓄怪前女友破坏运气,持恋爱期间偷拍性爱视频敲诈前女友2000元;其已被判拘役3个月并处罚金2000元。',\n",
       " '海口:数百名村民“围攻”城管,致20余人受伤,疑村民有组织有预谋阻碍执法。',\n",
       " '南安蓉溪村出现兽爪印,目击者称野兽身上有花斑,应该是豹子;当地几十年前豹满为患,曾出动部队清理',\n",
       " '哈尔滨小伙远赴温州见女网友,发现对方身材相貌与照片判若两人,遂对其当街暴打,并于当日乘飞机离去',\n",
       " '9名遭菲扣押中国渔民昨日获释,被控非法捕捞海龟,因无力付罚金,在菲坐牢整1年。渔民牢中照片',\n",
       " '奥巴马昨日宣誓就职美国总统并发表演讲,重申“人人平等而自由”的美国精神,继续支持世界民主',\n",
       " '“打工皇帝”杨元庆年薪达1.33亿元,较上一年大幅上涨46%;联想回应称很合理。',\n",
       " '第一季度福特降低在华产量,以应对对手价格战和汽车市场的趋缓;4月长安福特大部分车型销量都同比下降。']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(raw_datasets[\"test\"][\"summary\"][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c0c3c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "gold_list = []\n",
    "for text in raw_datasets[\"test\"][\"document\"][0:100]:\n",
    "    pred_list.append(generate_summary(text, model)[1][0])\n",
    "gold_list = list(raw_datasets[\"test\"][\"summary\"][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "592fc714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Collecting lawrouge\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/cc/9c/cc411fd95b5fdf1924b2336f33d1eb304b65c029d77666315d4d8ff8ba0b/lawrouge-2.0.0.tar.gz (8.5 kB)\n",
      "Building wheels for collected packages: lawrouge\n",
      "  Building wheel for lawrouge (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lawrouge: filename=lawrouge-2.0.0-py3-none-any.whl size=9289 sha256=de9df0aaaabcba163b251af9c1ecfb236f58debf9d1b470430f74b09822f5daf\n",
      "  Stored in directory: /home/ma-user/.cache/pip/wheels/89/d6/bd/fafe523360e3233e6a58600caf802538dc04171fb32423f665\n",
      "Successfully built lawrouge\n",
      "Installing collected packages: lawrouge\n",
      "Successfully installed lawrouge-2.0.0\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/ma-user/anaconda3/envs/PyTorch-1.4/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install lawrouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2577f925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lawrouge\n",
    "rouge = lawrouge.Rouge()\n",
    "score = rouge.get_scores([\"他是清华大学计算机科学与技术系。计算机科学与技术专业。\"], [\"他是清华大学计算机科学与技术系。\"], avg=0)\n",
    "score[0][\"rouge-2\"][\"r\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d157824",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "90819ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted score:  0.45293061867921885\n"
     ]
    }
   ],
   "source": [
    "import lawrouge\n",
    "rouge = lawrouge.Rouge()\n",
    "score_s = 0\n",
    "for i in range(len(gold_list)):\n",
    "    score = rouge.get_scores([pred_list[i].replace(\" \", \"\")], [gold_list[i].replace(\" \", \"\")], avg=0)\n",
    "    score_s += score[0][\"rouge-l\"][\"r\"]\n",
    "score_ave = score_s/len(gold_list)\n",
    "print('weighted score: ', score_ave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a8c0ab56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[unused2][夜宵]章子怡示爱汪峰被恶搞爆粗口回击:扯nmb;谢娜当众掐黄磊女儿脖子被批:没素质没爱心;周杰伦女友昆凌:看新闻才知婚讯。女友:我也是看了新闻...(组图)。))知情情案。详细也是新闻:看了看了)。看了。看。组图合同[unused2]'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_list[1].replace(\" \", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d612fec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f': 0.6274782560840833, 'p': 0.52, 'r': 0.7911111111111112}\n"
     ]
    }
   ],
   "source": [
    "rouge = lawrouge.Rouge()\n",
    "scores = rouge.get_scores([\"他是清华大学计算机科学与技术系\"], [\"计算机科学与技术专业\"], avg=2)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3625a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets[\"test\"][\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28adf101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moxing as mox\n",
    "mox.file.copy('/home/ma-user/work/obs_file.txt', 'obs://bart-summ/obs_file.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.4",
   "language": "python",
   "name": "pytorch-1.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
